from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import joblib
from nltk.tokenize import word_tokenize
import pymorphy3
import nltk
import uvicorn
import nltk

try:
    nltk.data.find('tokenizers/punkt')
    print("punkt tokenizer is available!")
except LookupError:
    print("punkt tokenizer not found. Downloading...")
    nltk.download('punkt')

try:
    nltk.data.find('tokenizers/punkt_tab')
    print("punkt_tab resource is available!")
except LookupError:
    print("punkt_tab resource not found. Downloading...")
    nltk.download('punkt_tab')

# --- –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è FastAPI ---
app = FastAPI(
    title="API –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –¢–µ–∫—Å—Ç–∞",
    description="–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º XGBoost"
)

# --- –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ ---
try:
    nltk.data.find("tokenizers/punkt")
except LookupError:
    nltk.download("punkt")

# --- –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ä–µ—Å—É—Ä—Å–æ–≤ ---
model = None
label_encoder = None
punctuation_marks_api = []
stop_words_api = []
morph_api = None

model_filename = "xgboost_text_classifier.joblib"

try:
    model_data = joblib.load(model_filename)
    model = model_data['model_pipeline']
    label_encoder = model_data['label_encoder']
    punctuation_marks_api = model_data['punctuation_marks']
    stop_words_api = model_data['stop_words']
    morph_api = pymorphy3.MorphAnalyzer()
    print(f"‚úÖ –ú–æ–¥–µ–ª—å '{model_filename}' –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ.")
except FileNotFoundError:
    print(f"‚ùå –§–∞–π–ª –º–æ–¥–µ–ª–∏ '{model_filename}' –Ω–µ –Ω–∞–π–¥–µ–Ω.")
except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")

# --- –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ ---
def preprocess_for_api(text):
    if model is None or morph_api is None or stop_words_api is None or punctuation_marks_api is None:
        print("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏: –†–µ—Å—É—Ä—Å—ã –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã.")
        return ""

    if isinstance(text, str):
        tokens = word_tokenize(text.lower())
        preprocessed_text = []
        for token in tokens:
            if token and token not in punctuation_marks_api and token not in stop_words_api:
                try:
                    lemma = morph_api.parse(token)[0].normal_form
                    preprocessed_text.append(lemma)
                except:
                    continue
        return ' '.join(preprocessed_text)
    return ""

# --- Pydantic –º–æ–¥–µ–ª–∏ ---
class TextInput(BaseModel):
    text: str

class ClassificationResult(BaseModel):
    direction: str
    probabilities: dict = None

# --- –≠–Ω–¥–ø–æ–∏–Ω—Ç—ã ---
@app.get("/")
def read_root():
    return {"message": "–î–æ–±—Ä–æ –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å –≤ API –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –¢–µ–∫—Å—Ç–∞. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ POST /classify –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏."}

@app.post("/classify", response_model=ClassificationResult)
def classify_text(input_data: TextInput):
    if model is None or label_encoder is None:
        raise HTTPException(status_code=500, detail="–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞.")

    processed_text = preprocess_for_api(input_data.text)
    if not processed_text:
        raise HTTPException(status_code=400, detail="–ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ç–µ–∫—Å—Ç.")

    try:
        pred_encoded = model.predict([processed_text])[0]

        if pred_encoded not in range(len(label_encoder.classes_)):
            raise ValueError("–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –∏–Ω–¥–µ–∫—Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è.")

        prediction = label_encoder.inverse_transform([pred_encoded])[0]

        probabilities = {}
        if hasattr(model, 'predict_proba'):
            try:
                probs = model.predict_proba([processed_text])[0]
                for i, cls in enumerate(label_encoder.classes_):
                    probabilities[cls] = float(probs[i])
            except:
                probabilities = None
        else:
            probabilities = None

        return ClassificationResult(direction=prediction, probabilities=probabilities)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {e}")

@app.get("/model_info")
def get_model_info():
    if model is None or label_encoder is None:
        raise HTTPException(status_code=500, detail="–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞.")
    return {
        "classes": label_encoder.classes_.tolist(),
        "model_type": "XGBoost + TF-IDF",
        "preprocessing": "–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è, —É–¥–∞–ª–µ–Ω–∏–µ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏/—Å—Ç–æ–ø-—Å–ª–æ–≤, –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è (pymorphy3)"
    }

# --- –ó–∞–ø—É—Å–∫ FastAPI –Ω–∞ –ª–æ–∫–∞–ª—å–Ω–æ–º —Å–µ—Ä–≤–µ—Ä–µ ---
if __name__ == "__main__":
    try:
        uvicorn.run(app, host="127.0.0.1", port=6666)
        print("üöÄ FastAPI –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –∑–∞–ø—É—â–µ–Ω–æ –Ω–∞ localhost:6666")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ FastAPI: {e}")
